1. N-gram Language Models ***assign probabilities to word sequences based on the occurrence of n-grams, which are sequences of n words.***
    
    - ***Language models are probabilistic models*** that assign probabilities to sequences of words.
    - They are used in various natural language processing tasks such as speech recognition, spelling correction, machine translation, and text generation.
2. N-gram Language Models:
    
    - N-gram language models are a ***specific type of language model.***
    - An N-gram is ***a sequence of N words.*** For example, a bigram (2-gram) consists of two words, and a trigram (3-gram) consists of three words.
    - N-gram models ***approximate the probability of the last word in an N-gram given the previous words.***
    - The assumption behind N-gram models is that the ***probability of a word depends only on the previous N-1 words (Markov assumption).***
3. Estimating N-gram Probabilities:
    
    - One common method to estimate N-gram probabilities is ***through relative frequency counts.***
    - By analyzing a large corpus of text, the number of occurrences of an N-gram can be counted, as well as the number of times it is followed by a specific word.
    - The probability of a word $w$ given a history $h$ can be estimated as the count of the $N-gram (h, w)$ divided by the count of the $N-gram (h)$.
4. Chain Rule of Probability:
    
    - The chain rule of probability allows us to ***compute the probability of an entire word sequence*** by ***decomposing it into a product of conditional probabilities.***
    - For example, the probability of a sequence w1, w2, ..., wn can be computed as P(w1) * P(w2|w1) * P(w3|w1:w2) * ... * P(wn|w1:w2:...:wn-1).
5. Challenges in Estimating N-gram Probabilities:
    
    - Estimating probabilities directly from counts ***may not work well in all cases, as language is creative and constantly evolving.***
    - The web, although vast, ***might not have enough data to provide accurate estimates*** for all possible N-grams.
    - Some N-grams ***may have zero counts***, making it challenging to estimate their probabilities accurately.
6. Simplification of N-gram Terminology:

    - In N-gram models, the term "N-gram" can ***refer to both the word sequence itself and the predictive model that assigns it a probability.***
    - For example, a bigram can refer to a two-word sequence like "please turn" or the model that assigns a probability to that sequence.
    - For instance, a bigram model approximates the probability of a word given all preceding words using only the conditional probability of the preceding word.
7. N-gram Language Models have various applications, including speech recognition, spelling correction, machine translation, and augmentative and alternative communication systems.
    
8. They ***serve as a foundational tool for understanding language modeling concepts***, despite being simpler compared to advanced neural language models.
