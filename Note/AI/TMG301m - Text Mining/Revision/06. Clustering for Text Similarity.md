# Clustering Overview
> A process in which data points are grouped together based on their similarities or distances in a multi-dimensional space. 
> The goal of clustering is to ***find natural groupings or patterns*** within a dataset, allowing similar data points to belong to the same cluster while being distinct from other clusters.

- Significance of Clustering in Unstructured Data 
	- Clustering is ***particularly significant when dealing with unstructured data***, such as text, images, or other forms of data where the underlying structure is not explicitly defined. 
	- In unstructured data, patterns and relationships are often ***hidden***, making it challenging to organize and extract meaningful insights. 
	- Clustering helps uncover these hidden patterns by grouping similar data points together, enabling the identification of categories, topics, trends, or groups that may not be immediately apparent.
# Clustering for Text Similarity
## How Clustering Algorithms Group Similar Text Documents
- Vector Representation: 
	- Each text document is represented as a numerical vector in a high-dimensional space. 
	- Common methods include using term frequency-inverse document frequency (TF-IDF) or word embeddings to convert words into numerical values.
- Distance Calculation: 
	- Clustering algorithms compute the distance between document vectors to determine their similarity. 
	- Documents with smaller distances are considered more similar to each other.
- Assignment to Clusters: 
	- The algorithm assigns each document to the cluster with the nearest centroid (average vector) or based on a similarity threshold. 
	- Documents that are closer in vector space are grouped together.
- Iterative Refinement: 
	- Depending on the algorithm, the process might be iterative. 
	- The centroids are recalculated, documents are reassigned, and the clusters are refined in each iteration.
# Distance Metrics for Text
## Role of Distance Metrics in Measuring Text Similarity
- Euclidean Distance: 
	- Measures the straight-line distance between two vectors in the space. 
	- It's sensitive to differences in both magnitude and direction.
	- Formula: $$\text{Euclidean Distance} = \sqrt{\sum\limits_{i=1}^{n}(x_{i}-y_{i})^{2}}$$
	- Range: Values non-negative and larger values indicates greater dissimilarity
- Cosine Similarity: 
	- Measures the cosine of the angle between two vectors. 
	- It's often used with TF-IDF vectors and is insensitive to vector magnitude.
	- Formula: $$\text{Cosine similarity} = \frac{A\cdot B}{\lVert A \rVert \lVert B \rVert}$$
	- Range: -1 to 1
- Jaccard Similarity: 
	- Measures the size of the intersection of two sets divided by the size of their union. 
	- It's often used for measuring similarity between sets of words.
	- Formula: $$\text{Jaccard Similarity} = \frac{\lvert A \cap B \rvert}{\lvert A \cup B \rvert }$$
- Pearson Correlation: 
- Measures the linear correlation between two vectors. 
- It captures both magnitude and direction.
# K-Means Clustering
- Widely used unsupervised machine learning algorithm for partitioning a dataset into a predetermined number of clusters. 
- It's particularly useful for text clustering, where the goal is to group similar text documents into distinct clusters based on their content.
- Avantages of K-Means 
	- K-Means is simple, easy to understand, and computationally efficient, making it suitable for large datasets.
	- It's effective for cases where clusters are spherical and similarly sized.
	- It provides insights into data grouping and can help uncover patterns within text data.

# Hierarchical Clustering
## Concept
- Hierarchical clustering is a method used to create a hierarchy of clusters in a dataset. 
- Unlike K-Means, which requires specifying the number of clusters beforehand, hierarchical clustering builds a tree-like structure of clusters, allowing for more nuanced insights into data relationships. 
- It doesn't just assign data points to clusters; it arranges them in a hierarchy that can be visualized as a dendrogram.
## Agglomerative and Divisive Hierarchical Clustering
- Agglomerative (Bottom-Up):
	- Agglomerative hierarchical clustering starts by treating each data point as an individual cluster.
	- It iteratively merges the closest clusters into larger ones, moving up the hierarchy until all data points belong to a single cluster.
- Divisive (Top-Down):
	- Divisive hierarchical clustering starts with all data points in a single cluster.
	- It recursively divides the clusters into smaller ones, moving down the hierarchy until each data point forms its own cluster.
# Density-based Clustering
## Concept
- Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is a popular density-based clustering algorithm used to discover clusters of arbitrary shapes in datasets. 
- Unlike K-Means or hierarchical clustering, which assume that clusters have similar sizes and shapes, DBSCAN can identify clusters with varying shapes and densities.
## DBSCAN Groups Data Points
The algorithm classifies data points into three categories:
- Core Points:
	- A core point is a data point that has a minimum number of neighboring points (specified by the "minPts" parameter) within a specified radius ("eps").
	- Core points are central to clusters and form the starting points for building clusters.
- Border Points:
	- A border point is a data point that has fewer neighboring points than "minPts" within the radius "eps" but falls within the radius of a core point.
	- Border points are on the edges of clusters and can connect clusters if they are near multiple core points.
- Noise Points (Outliers):
	- A noise point (outlier) is a data point that is neither a core nor a border point.
	- Noise points are considered to be part of no cluster and are often considered as unstructured or less relevant data.

# Text Pre-Processing for Clustering