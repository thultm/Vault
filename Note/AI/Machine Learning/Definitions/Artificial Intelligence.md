> " It is the science and engineering of making intelligent machines, especially intelligent computer programs. It is related to the similar task of using computers to understand human intelligence, but AI does not have to confine itself to methods that are biologically observable."
\ - John McCarthy (2004)
# [[Turing Test]]
> here a human interrogator would try to distinguish between a computer and human text response.
##  Four potential goals or definitions of AI 
> Which differentiates computer systems on the basis of rationality and thinking vs. acting.
- Human approach:
	- Systems that think like humans
	- Systems that act like humans
- Ideal approach:
	- Systems that think rationally
	- Systems that act rationally
Alan Turing’s definition would have fallen under the category of “systems that act like humans.”
# Types of [[Artificial Intelligence]] 
## [[Weak AI]] 
- Also called Narrow AI or [[Artificial Narrow Intelligence]] (ANI)—is AI trained and focused to perform specific tasks.
- Weak AI drives most of the AI that surrounds us today.
[[Strong AI]]
- Is made up of[[ Artificial General Intelligence]] (AGI) and [[Artificial Super Intelligence]] (ASI).
- AGI is a theoretical form of AI where a machine would have an intelligence equaled to humans.
- ASI, also known as superintelligence—would surpass the intelligence and ability of the human brain.
# [[Deep learning]] vs. [[Machine learning]]
-  Both [[Deep learning]]  and [[Machine learning]]  are sub-fields of [[Artificial Intelligence]] .
-  [[Deep learning]]  is actually a sub-field of [[Machine learning]]. 
# History of [[Artificial Intelligence]]
- **1950:** Alan Turing publishes _Computing Machinery and Intelligence._ In the paper, Turing—famous for breaking the Nazi's ENIGMA code during WWII—proposes to answer the question 'can machines think?' and introduces the Turing Test to determine if a computer can demonstrate the same intelligence (or the results of the same intelligence) as a human. The value of the Turing test has been debated ever since.

- **1956:** John McCarthy coins the term 'artificial intelligence' at the first-ever AI conference at Dartmouth College. (McCarthy would go on to invent the Lisp language.) Later that year, Allen Newell, J.C. Shaw, and Herbert Simon create the Logic Theorist, the first-ever running AI software program.

- **1967:** Frank Rosenblatt builds the Mark 1 Perceptron, the first computer based on a neural network that 'learned' though trial and error. Just a year later, Marvin Minsky and Seymour Papert publish a book titled _Perceptrons_, which becomes both the landmark work on neural networks and, at least for a while, an argument against future neural network research projects.

- **1980s:** Neural networks which use a backpropagation algorithm to train itself become widely used in AI applications.

- **1997:** IBM's Deep Blue beats then world chess champion Garry Kasparov, in a chess match (and rematch).

- **2011:** IBM Watson beats champions Ken Jennings and Brad Rutter at _Jeopardy!_

- **2015:** Baidu's Minwa supercomputer uses a special kind of deep neural network called a convolutional neural network to identify and categorize images with a higher rate of accuracy than the average human.

- **2016:** DeepMind's AlphaGo program, powered by a deep neural network, beats Lee Sodol, the world champion Go player, in a five-game match. The victory is significant given the huge number of possible moves as the game progresses (over 14.5 trillion after just four moves!). Later, Google purchased DeepMind for a reported USD 400 million.

- **2023**: A rise in large language models, or LLMs, such as ChatGPT, create an  enormous change in performance of AI and its potential to drive enterprise value.  With these new generative AI practices, deep-learning models can be pre-trained on  vast amounts of raw, unlabeled data.