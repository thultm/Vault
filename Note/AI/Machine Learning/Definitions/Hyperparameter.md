In [[machine learning]], a **hyperparameter** is a [[Parameter]] whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are derived via training.

**Hyperparameters** can be classified as model hyperparameters, that cannot be inferred while fitting the machine to the training set because they refer to the [[Model selection]] task, or algorithm hyperparameters, that in principle have no influence on the performance of the model but affect the speed and quality of the learning process. An example of a model hyperparameter is the topology and size of a [[Neural network]]. Examples of algorithm hyperparameters are [[learning rate]] and [[Batch size]] as well as [[Mini-batch size]]. [[Batch size]] can refer to the full data sample where [[Mini-batch size]] would be a smaller sample set.

Different model training algorithms require different **hyperparameters**, some simple algorithms (such as [[Ordinary least squares regression]]) require none. Given these **hyperparameters**, the training algorithm learns the [[parameters]] from the data. For instance, [[LASSO]] is an algorithm that adds a [[Regularization]] hyperparameter to [[Ordinary least squares regression]], which has to be set before estimating the [[parameters]] through the training algorithm.