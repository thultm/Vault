- Triton Inference Server can cater to the different requirements of different models.
- Link: [Triton Tutorials](https://github.com/triton-inference-server/tutorials/tree/main/Conceptual_Guide)
# Part 1: Model Deployment
## I. step-by-step
### 1. Create models
- Can uses TensorFlow, PyTorch, TensorRT, ONNX models and more.
- Recommended use ONNX format.
### 2. Setting up the model repository
- These model repositories can live in a local or network attached filesystem, or in a cloud object store like AWS S3, Azure Blob Storage or Google Cloud Storage.
- Servers can use also multiple different model repositories.
- Docs: [Model Repository Locations](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_repository.html#model-repository-locations "Permalink to this headline")
- Example
	```json
	# Example repository structure
	<model-repository>/
	  <model-name>/
	    [config.pbtxt]
	    [<output-labels-file> ...]
	    <version>/
	      <model-definition-file>
	    <version>/
	      <model-definition-file>
	    ...
	  <model-name>/
	    [config.pbtxt]
	    [<output-labels-file> ...]
	    <version>/
	      <model-definition-file>
	    <version>/
	      <model-definition-file>
	    ...
	  ...
	```
	There are three important components to be discussed from the above structure:
	- `model-name`: The identifying name for the model.
	- `config.pbtxt`: For each model, users can define a model configuration. This configuration, at minimum, needs to define: the backend, name, shape, and datatype of model inputs and outputs. For most of the popular backends, this configuration file is autogenerated with defaults. The full specification of the configuration file can be found in the [`model_config` protobuf definition](https://github.com/triton-inference-server/common/blob/main/protobuf/model_config.proto).
	- `version`: versioning makes multiple versions of the same model available for use depending on the policy selected. [More Information about versioning.](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_repository.html#model-versions)
	
	For this example you can set up the model repository structure in the following manner: 
	```shell
	mkdir -p model_repository/text_detection/1
	mv detection.onnx model_repository/text_detection/1/model.onnx
	
	mkdir -p model_repository/text_recognition/1
	mv str.onnx model_repository/text_recognition/1/model.onnx
	```
	These commands should give you a repository that looks this:
	```json
	# Expected folder layout
	model_repository/
	├── text_detection
	│   ├── 1
	│   │   └── model.onnx
	│   └── config.pbtxt
	└── text_recognition
	    ├── 1
	    │   └── model.onnx
	    └── config.pbtxt
	```
	Note that, for this example, we've already created the `config.pbtxt` files and placed them in the necessary location. In the next section, we'll discuss the contents of these files.
### 3. Model configuration
With the models and the file structure ready, the next things we need to look at are the `config.pbtxt` model configuration files.

Let's first look at the model configuration for the `EAST text detection` model that's been provided for you at `/model_repository/text_detection/config.pbtxt`
```protobuf
name: "text_detection"
backend: "onnxruntime"
max_batch_size : 256
input [
  {
    name: "input_images:0"
    data_type: TYPE_FP32
    dims: [ -1, -1, -1, 3 ]
  }
]
output [
  {
    name: "feature_fusion/Conv_7/Sigmoid:0"
    data_type: TYPE_FP32
    dims: [ -1, -1, -1, 1 ]
  }
]
output [
  {
    name: "feature_fusion/concat_3:0"
    data_type: TYPE_FP32
    dims: [ -1, -1, -1, 5 ]
  }
]
```
- `name`: "name" is an optional field, the value of which should match the name of the directory of the model.
- `backend`: This field indicates which backend is being used to run the model. Triton supports a wide variety of backends like TensorFlow, PyTorch, Python, ONNX and more. For a complete list of field selection refer to [these comments](https://github.com/triton-inference-server/backend#backends).
- `max_batch_size`: As the name implies, this field defines the maximum batch size that the model can support.
- `input` and `output`: The input and output sections specify the name, shape, datatype, and more, while providing operations like [reshaping](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md#reshape) and support for [ragged batches](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/ragged_batching.md#ragged-batching).

In [most cases](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_configuration.html#auto-generated-model-configuration), it's possible to leave out the `input` and `output` sections and let Triton extract that information from the model files directly.

For details of all supported fields and their values, refer to the [model config protobuf definition file](https://github.com/triton-inference-server/common/blob/main/protobuf/model_config.proto).
### 4. Launching the server
While the Triton Inference Server can be [built from source](https://github.com/triton-inference-server/server/blob/main/docs/customization_guide/build.md#building-triton), the use of [pre-built Docker containers](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver) freely available from NGC is highly recommended for this example.
```shell
# Replace the yy.mm in the image name with the release year and month
# of the Triton version needed, eg. 22.08

docker run --gpus=all -it --shm-size=256m --rm -p8000:8000 -p8001:8001 -p8002:8002 -v $(pwd)/model_repository:/models nvcr.io/nvidia/tritonserver:<yy.mm>-py3
```
Once Triton Inference Server has been built or once inside the container, it can be launched with the command:
```shell
tritonserver --model-repository=/models
```
This will spin up the server and model instances will be ready for inference.
## II. Building a client application
There are three ways to interact with the Triton Inference Server:
- HTTP(S) API
- gRPC API
- Native C API
There are also pre-built [client libraries](https://github.com/triton-inference-server/client#client-library-apis) in [C++](https://github.com/triton-inference-server/client/tree/main/src/c%2B%2B), [Python](https://github.com/triton-inference-server/client/tree/main/src/python), and [Java](https://github.com/triton-inference-server/client/tree/main/src/java) that wrap over the HTTP and gRPC APIs. This example contains a Python client script in `client.py` which uses the `tritonclient` python library to communicate with Triton over the HTTP API.
# Optimizing Triton configuration
## 1. Performance Discussion
![](https://i.imgur.com/zmWnkLf.png)

Khi có query từ client, Triton Server sẽ cho query vào queue. Khi 1 trong các model instances được giải phóng, 1 dynamic batch có kích thước tương ứng với batch size đã cấu hình sẵn được cấu thành bởi các query hiện tại, query tiếp tục được gửi đến. Batch này sau đó sẽ được convert sang format mà các Framework Backends và được gửi tới đó để xử lý. Sau khi inference thì kết quả sẽ được trả về client thông qua C API.

Có 3 nguyên nhân chính gây ra độ trễ:
- [[Network Latency]] (Độ trễ mạng)
	- Minimizing [[Network latency]] is a case by case process.
- [[Inference Compute time]] (Độ trễ lúc tính toán suy luận)
	- Accelerating models to trim down the effective [[Inference Compute time|compute time]] is commonly achieved with a plethora of techniques like: optimizing the network graphs by fusing layers, reducing models precision, fusing kernels and more!
- Latency caused due to wait time in the model's queue (Độ trễ do thời gian chờ đợi)
	- Latency in **queues** can primarily be addressed by adding more instances of the model.
	- To streamline this experience, Triton Inference Server comes with [[Model Analyzer]].
